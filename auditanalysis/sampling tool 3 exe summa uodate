import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import math
import random
from datetime import datetime, timedelta
import os
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import seaborn as sns
import io

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class OMRCRiskBasedSamplingTool:
    def __init__(self, root):
        self.root = root
        self.root.title("OMRC Enhanced Risk-Based Sampling Tool - HBAP, HBEU, HBUS")
        self.root.geometry("1600x1000")
        self.root.configure(bg='#f0f0f0')
        
        # Data variables
        self.data = None
        self.sample_data = None
        self.comparison_results = {}
        
        # Dynamic statistical weights (calculated from data)
        self.product_weights = {}
        self.reason_code_weights = {}
        self.entity_risk_scores = {}
        self.regional_risk_scores = {}
        self.additional_risk_weights = {}  # For optional additional columns
        
        # Store stratum allocation for analysis
        self.last_stratum_samples = {}
        self.sampling_parameters = {}
        self.stratum_details = {}
        
        # Create UI
        self.create_widgets()
        
    def safe_float_conversion(self, value, default=0.0):
        """Safely convert value to float with fallback"""
        try:
            if isinstance(value, str):
                value = value.strip()
                if value == '':
                    return default
            return float(value)
        except (ValueError, TypeError):
            return default
    
    def safe_int_conversion(self, value, default=0):
        """Safely convert value to int with fallback"""
        try:
            if isinstance(value, str):
                value = value.strip()
                if value == '':
                    return default
            return int(float(value))  # Convert through float first to handle decimals
        except (ValueError, TypeError):
            return default
    
    def ensure_numeric_column(self, df, column_name):
        """Ensure DataFrame column is numeric, converting if necessary"""
        if column_name in df.columns:
            df[column_name] = pd.to_numeric(df[column_name], errors='coerce')
        return df
    
    def safe_sort_unique(self, series):
        """Safely sort unique values, handling mixed types"""
        try:
            unique_vals = series.dropna().unique()
            # Try to sort normally first
            try:
                return sorted(unique_vals)
            except TypeError:
                # If normal sort fails, convert all to strings and sort
                return sorted([str(val) for val in unique_vals])
        except Exception:
            return list(series.dropna().unique())
        
    def create_widgets(self):
        # Create notebook for tabs
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill='both', expand=True, padx=10, pady=10)
        
        # Tab 1: Data Loading and Configuration
        self.tab1 = ttk.Frame(notebook)
        notebook.add(self.tab1, text="Data & Configuration")
        
        # Tab 2: Risk Calculation Setup
        self.tab2 = ttk.Frame(notebook)
        notebook.add(self.tab2, text="Risk Calculation")
        
        # Tab 3: Sampling Comparison
        self.tab3 = ttk.Frame(notebook)
        notebook.add(self.tab3, text="Sampling Comparison")
        
        # Tab 4: Results and Analysis
        self.tab4 = ttk.Frame(notebook)
        notebook.add(self.tab4, text="Results & Analysis")
        
        # Tab 5: Visualizations
        self.tab5 = ttk.Frame(notebook)
        notebook.add(self.tab5, text="Visualizations")
        
        self.create_tab1_widgets()
        self.create_tab2_widgets()
        self.create_tab3_widgets()
        self.create_tab4_widgets()
        self.create_tab5_widgets()
        
    def create_tab1_widgets(self):
        """Tab 1: Data Loading and Configuration - Enhanced with additional columns"""
        
        # Data Loading Frame
        data_frame = ttk.LabelFrame(self.tab1, text="Data Loading", padding="10")
        data_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        ttk.Button(data_frame, text="Load Exception Data", 
                  command=self.load_data).grid(row=0, column=0, padx=5)
        ttk.Button(data_frame, text="Generate Sample Data", 
                  command=self.generate_sample_data).grid(row=0, column=1, padx=5)
        
        self.data_label = ttk.Label(data_frame, text="No data loaded")
        self.data_label.grid(row=1, column=0, columnspan=2, pady=5)
        
        # Column Mapping Frame - Enhanced
        mapping_frame = ttk.LabelFrame(self.tab1, text="Column Mapping", padding="10")
        mapping_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        # Mandatory Columns Section
        ttk.Label(mapping_frame, text="MANDATORY COLUMNS:", font=('Arial', 10, 'bold')).grid(row=0, column=0, columnspan=4, sticky=tk.W, pady=(0,5))
        
        # Entity Column (Mandatory)
        ttk.Label(mapping_frame, text="Legal Entity Column:").grid(row=1, column=0, sticky=tk.W)
        self.entity_col_var = tk.StringVar(value="legal_entity")
        self.entity_col_combo = ttk.Combobox(mapping_frame, textvariable=self.entity_col_var, width=20)
        self.entity_col_combo.grid(row=1, column=1, padx=5, sticky=tk.W)
        
        # Region Column (Mandatory)
        ttk.Label(mapping_frame, text="Region Column:").grid(row=1, column=2, sticky=tk.W, padx=(20,0))
        self.region_col_var = tk.StringVar(value="region")
        self.region_col_combo = ttk.Combobox(mapping_frame, textvariable=self.region_col_var, width=20)
        self.region_col_combo.grid(row=1, column=3, padx=5, sticky=tk.W)
        
        # Product Column (Mandatory)
        ttk.Label(mapping_frame, text="Product Type Column:").grid(row=2, column=0, sticky=tk.W)
        self.product_col_var = tk.StringVar(value="product_type")
        self.product_col_combo = ttk.Combobox(mapping_frame, textvariable=self.product_col_var, width=20)
        self.product_col_combo.grid(row=2, column=1, padx=5, sticky=tk.W)
        
        # Reason Code Column (Mandatory)
        ttk.Label(mapping_frame, text="Reason Code Column:").grid(row=2, column=2, sticky=tk.W, padx=(20,0))
        self.reason_col_var = tk.StringVar(value="reason_code")
        self.reason_col_combo = ttk.Combobox(mapping_frame, textvariable=self.reason_col_var, width=20)
        self.reason_col_combo.grid(row=2, column=3, padx=5, sticky=tk.W)
        
        # Optional Additional Columns Section
        ttk.Label(mapping_frame, text="OPTIONAL ADDITIONAL STRATIFICATION COLUMNS:", font=('Arial', 10, 'bold')).grid(row=3, column=0, columnspan=4, sticky=tk.W, pady=(15,5))
        
        # Desk Column (Optional)
        ttk.Label(mapping_frame, text="Desk Column:").grid(row=4, column=0, sticky=tk.W)
        self.desk_col_var = tk.StringVar(value="None")
        self.desk_col_combo = ttk.Combobox(mapping_frame, textvariable=self.desk_col_var, width=20)
        self.desk_col_combo.grid(row=4, column=1, padx=5, sticky=tk.W)
        
        # Book Column (Optional)
        ttk.Label(mapping_frame, text="Book Column:").grid(row=4, column=2, sticky=tk.W, padx=(20,0))
        self.book_col_var = tk.StringVar(value="None")
        self.book_col_combo = ttk.Combobox(mapping_frame, textvariable=self.book_col_var, width=20)
        self.book_col_combo.grid(row=4, column=3, padx=5, sticky=tk.W)
        
        # Trader Column (Optional)
        ttk.Label(mapping_frame, text="Trader Column:").grid(row=5, column=0, sticky=tk.W)
        self.trader_col_var = tk.StringVar(value="None")
        self.trader_col_combo = ttk.Combobox(mapping_frame, textvariable=self.trader_col_var, width=20)
        self.trader_col_combo.grid(row=5, column=1, padx=5, sticky=tk.W)
        
        # Display Columns Section
        ttk.Label(mapping_frame, text="DISPLAY COLUMNS (Optional):", font=('Arial', 10, 'bold')).grid(row=6, column=0, columnspan=4, sticky=tk.W, pady=(15,5))
        
        # Trade Value Column (Display only)
        ttk.Label(mapping_frame, text="Trade Value Column:").grid(row=7, column=0, sticky=tk.W)
        self.value_col_var = tk.StringVar(value="trade_value")
        self.value_col_combo = ttk.Combobox(mapping_frame, textvariable=self.value_col_var, width=20)
        self.value_col_combo.grid(row=7, column=1, padx=5, sticky=tk.W)
        
        # Aging Days Column (Display only)
        ttk.Label(mapping_frame, text="Aging Days Column:").grid(row=7, column=2, sticky=tk.W, padx=(20,0))
        self.aging_col_var = tk.StringVar(value="aging_days")
        self.aging_col_combo = ttk.Combobox(mapping_frame, textvariable=self.aging_col_var, width=20)
        self.aging_col_combo.grid(row=7, column=3, padx=5, sticky=tk.W)
        
        # Data Preview with horizontal scrolling
        preview_frame = ttk.LabelFrame(self.tab1, text="Data Preview", padding="10")
        preview_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        
        # Treeview with both horizontal and vertical scrollbars
        self.tree = ttk.Treeview(preview_frame)
        scrollbar_y = ttk.Scrollbar(preview_frame, orient="vertical", command=self.tree.yview)
        scrollbar_x = ttk.Scrollbar(preview_frame, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=scrollbar_y.set, xscrollcommand=scrollbar_x.set)
        
        self.tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar_y.grid(row=0, column=1, sticky=(tk.N, tk.S))
        scrollbar_x.grid(row=1, column=0, sticky=(tk.W, tk.E))
        
        # Configure grid weights
        self.tab1.rowconfigure(2, weight=1)
        self.tab1.columnconfigure(0, weight=1)
        preview_frame.rowconfigure(0, weight=1)
        preview_frame.columnconfigure(0, weight=1)
        
    def create_tab2_widgets(self):
        """Tab 2: Risk Calculation Setup - Enhanced with additional column support"""
        
        # Risk Calculation Frame
        calc_frame = ttk.LabelFrame(self.tab2, text="Statistical Risk Calculation", padding="10")
        calc_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        ttk.Button(calc_frame, text="Calculate Statistical Risk Scores", 
                  command=self.calculate_statistical_risk_scores).grid(row=0, column=0, padx=5)
        
        self.risk_calc_label = ttk.Label(calc_frame, text="Risk scores not calculated")
        self.risk_calc_label.grid(row=0, column=1, padx=10)
        
        # Create notebook for risk score tables
        risk_notebook = ttk.Notebook(calc_frame)
        risk_notebook.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=10)
        
        # Entity Risk Scores Tab
        entity_frame = ttk.Frame(risk_notebook)
        risk_notebook.add(entity_frame, text="Entity Risk Scores")
        
        self.entity_tree = ttk.Treeview(entity_frame, columns=("Count", "Frequency", "Risk Score"), show="tree headings")
        self.entity_tree.heading("#0", text="Legal Entity")
        self.entity_tree.heading("Count", text="Exception Count")
        self.entity_tree.heading("Frequency", text="Frequency %")
        self.entity_tree.heading("Risk Score", text="Risk Score")
        self.entity_tree.column("#0", width=120)
        self.entity_tree.column("Count", width=100)
        self.entity_tree.column("Frequency", width=100)
        self.entity_tree.column("Risk Score", width=100)
        
        entity_scrollbar = ttk.Scrollbar(entity_frame, orient="vertical", command=self.entity_tree.yview)
        self.entity_tree.configure(yscrollcommand=entity_scrollbar.set)
        
        self.entity_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        entity_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Regional Risk Scores Tab
        region_frame = ttk.Frame(risk_notebook)
        risk_notebook.add(region_frame, text="Regional Risk Scores")
        
        self.region_tree = ttk.Treeview(region_frame, columns=("Count", "Frequency", "Risk Score"), show="tree headings")
        self.region_tree.heading("#0", text="Region")
        self.region_tree.heading("Count", text="Exception Count")
        self.region_tree.heading("Frequency", text="Frequency %")
        self.region_tree.heading("Risk Score", text="Risk Score")
        self.region_tree.column("#0", width=120)
        self.region_tree.column("Count", width=100)
        self.region_tree.column("Frequency", width=100)
        self.region_tree.column("Risk Score", width=100)
        
        region_scrollbar = ttk.Scrollbar(region_frame, orient="vertical", command=self.region_tree.yview)
        self.region_tree.configure(yscrollcommand=region_scrollbar.set)
        
        self.region_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        region_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Product Risk Scores Tab
        product_frame = ttk.Frame(risk_notebook)
        risk_notebook.add(product_frame, text="Product Risk Scores")
        
        self.product_tree = ttk.Treeview(product_frame, columns=("Count", "Frequency", "Risk Score"), show="tree headings")
        self.product_tree.heading("#0", text="Product Type")
        self.product_tree.heading("Count", text="Exception Count")
        self.product_tree.heading("Frequency", text="Frequency %")
        self.product_tree.heading("Risk Score", text="Risk Score")
        self.product_tree.column("#0", width=120)
        self.product_tree.column("Count", width=100)
        self.product_tree.column("Frequency", width=100)
        self.product_tree.column("Risk Score", width=100)
        
        product_scrollbar = ttk.Scrollbar(product_frame, orient="vertical", command=self.product_tree.yview)
        self.product_tree.configure(yscrollcommand=product_scrollbar.set)
        
        self.product_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        product_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Reason Code Risk Scores Tab
        reason_frame = ttk.Frame(risk_notebook)
        risk_notebook.add(reason_frame, text="Reason Code Risk Scores")
        
        self.reason_tree = ttk.Treeview(reason_frame, columns=("Count", "Frequency", "Risk Score"), show="tree headings")
        self.reason_tree.heading("#0", text="Reason Code")
        self.reason_tree.heading("Count", text="Exception Count")
        self.reason_tree.heading("Frequency", text="Frequency %")
        self.reason_tree.heading("Risk Score", text="Risk Score")
        self.reason_tree.column("#0", width=120)
        self.reason_tree.column("Count", width=100)
        self.reason_tree.column("Frequency", width=100)
        self.reason_tree.column("Risk Score", width=100)
        
        reason_scrollbar = ttk.Scrollbar(reason_frame, orient="vertical", command=self.reason_tree.yview)
        self.reason_tree.configure(yscrollcommand=reason_scrollbar.set)
        
        self.reason_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        reason_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Additional Columns Tab (if applicable)
        self.additional_frame = ttk.Frame(risk_notebook)
        risk_notebook.add(self.additional_frame, text="Additional Columns")
        
        self.additional_tree = ttk.Treeview(self.additional_frame, columns=("Count", "Frequency", "Risk Score"), show="tree headings")
        self.additional_tree.heading("#0", text="Additional Column")
        self.additional_tree.heading("Count", text="Exception Count")
        self.additional_tree.heading("Frequency", text="Frequency %")
        self.additional_tree.heading("Risk Score", text="Risk Score")
        self.additional_tree.column("#0", width=120)
        self.additional_tree.column("Count", width=100)
        self.additional_tree.column("Frequency", width=100)
        self.additional_tree.column("Risk Score", width=100)
        
        additional_scrollbar = ttk.Scrollbar(self.additional_frame, orient="vertical", command=self.additional_tree.yview)
        self.additional_tree.configure(yscrollcommand=additional_scrollbar.set)
        
        self.additional_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        additional_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Configure grid weights
        self.tab2.rowconfigure(1, weight=1)
        self.tab2.columnconfigure(0, weight=1)
        calc_frame.rowconfigure(1, weight=1)
        calc_frame.columnconfigure(0, weight=1)
        
    def create_tab3_widgets(self):
        """Tab 3: Sampling Comparison"""
        
        # Parameters Frame
        params_frame = ttk.LabelFrame(self.tab3, text="Sampling Parameters", padding="10")
        params_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        # Confidence Level
        ttk.Label(params_frame, text="Confidence Level:").grid(row=0, column=0, sticky=tk.W)
        self.confidence_var = tk.StringVar(value="95")
        confidence_combo = ttk.Combobox(params_frame, textvariable=self.confidence_var,
                                       values=["90", "95", "99"], width=10)
        confidence_combo.grid(row=0, column=1, padx=5, sticky=tk.W)
        
        # Margin of Error
        ttk.Label(params_frame, text="Margin of Error:").grid(row=0, column=2, sticky=tk.W, padx=(20,0))
        self.margin_var = tk.StringVar(value="0.05")
        margin_entry = ttk.Entry(params_frame, textvariable=self.margin_var, width=10)
        margin_entry.grid(row=0, column=3, padx=5, sticky=tk.W)
        
        # Risk Appetite
        ttk.Label(params_frame, text="Risk Appetite (p):").grid(row=1, column=0, sticky=tk.W)
        self.risk_var = tk.StringVar(value="0.15")
        risk_entry = ttk.Entry(params_frame, textvariable=self.risk_var, width=10)
        risk_entry.grid(row=1, column=1, padx=5, sticky=tk.W)
        
        # Anomaly Contamination
        ttk.Label(params_frame, text="Anomaly Contamination:").grid(row=1, column=2, sticky=tk.W, padx=(20,0))
        self.anomaly_var = tk.StringVar(value="0.10")
        anomaly_entry = ttk.Entry(params_frame, textvariable=self.anomaly_var, width=10)
        anomaly_entry.grid(row=1, column=3, padx=5, sticky=tk.W)
        
        # Methods Frame
        methods_frame = ttk.LabelFrame(self.tab3, text="Sampling Methods", padding="10")
        methods_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        self.method_vars = {}
        methods = [
            ('Traditional Random', 'traditional'),
            ('Statistical Risk-Based', 'risk_based'),
            ('Hybrid Enhanced', 'hybrid')
        ]
        
        for i, (name, key) in enumerate(methods):
            var = tk.BooleanVar(value=True)
            self.method_vars[key] = var
            ttk.Checkbutton(methods_frame, text=name, variable=var).grid(row=0, column=i, padx=20)
        
        # Generate Samples Button
        ttk.Button(methods_frame, text="Generate & Compare Samples", 
                  command=self.generate_comparison_samples).grid(row=1, column=0, columnspan=3, pady=20)
        
        # Results Frame with formatted output
        results_frame = ttk.LabelFrame(self.tab3, text="Comparison Results", padding="10")
        results_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        
        # Create notebook for results
        results_notebook = ttk.Notebook(results_frame)
        results_notebook.pack(fill='both', expand=True)
        
        # Summary Results Tab
        summary_frame = ttk.Frame(results_notebook)
        results_notebook.add(summary_frame, text="Summary")
        
        self.summary_tree = ttk.Treeview(summary_frame, columns=("Sample_Size", "High_Risk_Count", "Coverage", "Avg_Risk", "Entities", "Regions"), show="tree headings")
        self.summary_tree.heading("#0", text="Method")
        self.summary_tree.heading("Sample_Size", text="Sample Size")
        self.summary_tree.heading("High_Risk_Count", text="High-Risk Count")
        self.summary_tree.heading("Coverage", text="Coverage %")
        self.summary_tree.heading("Avg_Risk", text="Avg Risk Score")
        self.summary_tree.heading("Entities", text="Entities Covered")
        self.summary_tree.heading("Regions", text="Regions Covered")
        
        # Configure column widths
        for col in ["#0", "Sample_Size", "High_Risk_Count", "Coverage", "Avg_Risk", "Entities", "Regions"]:
            self.summary_tree.column(col, width=100)
        
        summary_scrollbar = ttk.Scrollbar(summary_frame, orient="vertical", command=self.summary_tree.yview)
        self.summary_tree.configure(yscrollcommand=summary_scrollbar.set)
        
        self.summary_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        summary_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Detailed Results Tab
        detail_frame = ttk.Frame(results_notebook)
        results_notebook.add(detail_frame, text="Detailed Results")
        
        self.results_text = tk.Text(detail_frame, height=20, width=80, font=('Courier', 10))
        results_scrollbar = ttk.Scrollbar(detail_frame, orient="vertical", command=self.results_text.yview)
        self.results_text.configure(yscrollcommand=results_scrollbar.set)
        
        self.results_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        results_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Configure weights
        self.tab3.rowconfigure(2, weight=1)
        self.tab3.columnconfigure(0, weight=1)
        
    def create_tab4_widgets(self):
        """Tab 4: Results and Analysis - Enhanced with out-of-scope export"""
        
        # Export Frame
        export_frame = ttk.LabelFrame(self.tab4, text="Export Options", padding="10")
        export_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        ttk.Button(export_frame, text="Export Samples to CSV", 
                  command=self.export_samples).grid(row=0, column=0, padx=5)
        ttk.Button(export_frame, text="Export Comparison Report", 
                  command=self.export_report).grid(row=0, column=1, padx=5)
        ttk.Button(export_frame, text="Export Risk Analysis", 
                  command=self.export_risk_analysis).grid(row=0, column=2, padx=5)
        ttk.Button(export_frame, text="Export Stratum Analysis", 
                  command=self.export_stratum_analysis).grid(row=1, column=0, padx=5)
        ttk.Button(export_frame, text="Export Out-of-Scope", 
                  command=self.export_out_of_scope_exceptions).grid(row=1, column=1, padx=5)
        
        # Analysis Notebook
        analysis_notebook = ttk.Notebook(self.tab4)
        analysis_notebook.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        
        # Executive Summary Tab - Formula and Parameters
        exec_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(exec_frame, text="Executive Summary")
        
        self.exec_text = tk.Text(exec_frame, height=25, width=80, font=('Courier', 11))
        exec_scrollbar = ttk.Scrollbar(exec_frame, orient="vertical", command=self.exec_text.yview)
        self.exec_text.configure(yscrollcommand=exec_scrollbar.set)
        
        self.exec_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        exec_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Stratum Analysis Tab - Detailed breakdown by stratum
        stratum_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(stratum_frame, text="Stratum Analysis")
        
        # Create Treeview for stratum details
        self.stratum_tree = ttk.Treeview(stratum_frame, columns=("Population", "Risk_Weight", "Base_Sample", "Risk_Adjusted", "Final_Sample"), show="tree headings")
        self.stratum_tree.heading("#0", text="Stratum (Enhanced Multi-Dimensional)")
        self.stratum_tree.heading("Population", text="Population")
        self.stratum_tree.heading("Risk_Weight", text="Risk Weight")
        self.stratum_tree.heading("Base_Sample", text="Base Sample")
        self.stratum_tree.heading("Risk_Adjusted", text="Risk Adjusted")
        self.stratum_tree.heading("Final_Sample", text="Final Sample")
        
        # Configure column widths
        self.stratum_tree.column("#0", width=350)
        for col in ["Population", "Risk_Weight", "Base_Sample", "Risk_Adjusted", "Final_Sample"]:
            self.stratum_tree.column(col, width=100)
        
        stratum_scrollbar_y = ttk.Scrollbar(stratum_frame, orient="vertical", command=self.stratum_tree.yview)
        stratum_scrollbar_x = ttk.Scrollbar(stratum_frame, orient="horizontal", command=self.stratum_tree.xview)
        self.stratum_tree.configure(yscrollcommand=stratum_scrollbar_y.set, xscrollcommand=stratum_scrollbar_x.set)
        
        self.stratum_tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        stratum_scrollbar_y.grid(row=0, column=1, sticky=(tk.N, tk.S))
        stratum_scrollbar_x.grid(row=1, column=0, sticky=(tk.W, tk.E))
        
        stratum_frame.rowconfigure(0, weight=1)
        stratum_frame.columnconfigure(0, weight=1)
        
        # Detailed Analysis Tab
        detail_analysis_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(detail_analysis_frame, text="Detailed Analysis")
        
        self.analysis_text = tk.Text(detail_analysis_frame, height=25, width=80, font=('Courier', 10))
        analysis_scrollbar = ttk.Scrollbar(detail_analysis_frame, orient="vertical", command=self.analysis_text.yview)
        self.analysis_text.configure(yscrollcommand=analysis_scrollbar.set)
        
        self.analysis_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        analysis_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Risk Breakdown Tab
        risk_breakdown_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(risk_breakdown_frame, text="Risk Breakdown")
        
        self.risk_breakdown_tree = ttk.Treeview(risk_breakdown_frame, columns=("Total", "High_Risk", "Percentage", "Avg_Risk"), show="tree headings")
        self.risk_breakdown_tree.heading("#0", text="Category")
        self.risk_breakdown_tree.heading("Total", text="Total Count")
        self.risk_breakdown_tree.heading("High_Risk", text="High-Risk Count")
        self.risk_breakdown_tree.heading("Percentage", text="High-Risk %")
        self.risk_breakdown_tree.heading("Avg_Risk", text="Avg Risk Score")
        
        for col in ["#0", "Total", "High_Risk", "Percentage", "Avg_Risk"]:
            self.risk_breakdown_tree.column(col, width=130)
        
        risk_breakdown_scrollbar = ttk.Scrollbar(risk_breakdown_frame, orient="vertical", command=self.risk_breakdown_tree.yview)
        self.risk_breakdown_tree.configure(yscrollcommand=risk_breakdown_scrollbar.set)
        
        self.risk_breakdown_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        risk_breakdown_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Configure weights
        self.tab4.rowconfigure(1, weight=1)
        self.tab4.columnconfigure(0, weight=1)
        
    def create_tab5_widgets(self):
        """Tab 5: Enhanced Visualizations"""
        
        # Control Frame
        control_frame = ttk.LabelFrame(self.tab5, text="Visualization Controls", padding="10")
        control_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=5)
        
        ttk.Button(control_frame, text="Update Charts", 
                  command=self.update_visualizations).grid(row=0, column=0, padx=5)
        
        # Chart selection
        ttk.Label(control_frame, text="Chart Type:").grid(row=0, column=1, padx=10)
        self.chart_type = tk.StringVar(value="comprehensive")
        chart_combo = ttk.Combobox(control_frame, textvariable=self.chart_type,
                                  values=["comprehensive", "coverage", "distribution", "risk_analysis", "stratum_heatmap"], width=15)
        chart_combo.grid(row=0, column=2, padx=5)
        
        # Visualization Frame
        viz_frame = ttk.LabelFrame(self.tab5, text="Enhanced Charts", padding="10")
        viz_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        
        # Matplotlib figure - Larger for better visualizations
        self.fig, self.axes = plt.subplots(2, 3, figsize=(18, 12))
        self.fig.tight_layout(pad=3.0)
        
        self.canvas = FigureCanvasTkAgg(self.fig, master=viz_frame)
        self.canvas.draw()
        self.canvas.get_tk_widget().grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Configure weights
        self.tab5.rowconfigure(1, weight=1)
        self.tab5.columnconfigure(0, weight=1)
        viz_frame.rowconfigure(0, weight=1)
        viz_frame.columnconfigure(0, weight=1)
        
    def load_data(self):
        """Load exception data from CSV file"""
        
        file_path = filedialog.askopenfilename(
            title="Select Exception Data File",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx"), ("All files", "*.*")]
        )
        
        if file_path:
            try:
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path)
                else:
                    df = pd.read_excel(file_path)
                
                self.data = df
                self.update_column_dropdowns()
                self.update_data_preview()
                self.data_label.config(text=f"Loaded {len(df):,} records from {os.path.basename(file_path)}")
                
                messagebox.showinfo("Success", f"Loaded {len(df):,} records successfully")
                
            except Exception as e:
                messagebox.showerror("Error", f"Failed to load data: {str(e)}")
    
    def update_column_dropdowns(self):
        """Update column dropdown menus with loaded data columns - Enhanced"""
        
        if self.data is None:
            return
        
        columns = ["None"] + list(self.data.columns)
        
        # Update all dropdowns
        self.entity_col_combo['values'] = columns
        self.region_col_combo['values'] = columns
        self.product_col_combo['values'] = columns
        self.reason_col_combo['values'] = columns
        self.desk_col_combo['values'] = columns
        self.book_col_combo['values'] = columns
        self.trader_col_combo['values'] = columns
        self.value_col_combo['values'] = columns
        self.aging_col_combo['values'] = columns
        
        # Try to auto-detect common column names
        for col in self.data.columns:
            col_lower = col.lower()
            if 'entity' in col_lower or 'legal' in col_lower:
                self.entity_col_var.set(col)
            elif 'region' in col_lower or 'hub' in col_lower:
                self.region_col_var.set(col)
            elif 'product' in col_lower or 'type' in col_lower:
                self.product_col_var.set(col)
            elif 'reason' in col_lower or 'code' in col_lower:
                self.reason_col_var.set(col)
            elif 'desk' in col_lower:
                self.desk_col_var.set(col)
            elif 'book' in col_lower:
                self.book_col_var.set(col)
            elif 'trader' in col_lower:
                self.trader_col_var.set(col)
            elif 'value' in col_lower or 'amount' in col_lower or 'notional' in col_lower:
                self.value_col_var.set(col)
            elif 'aging' in col_lower or 'days' in col_lower or 'age' in col_lower:
                self.aging_col_var.set(col)
    
    def get_stratification_columns(self):
        """Get all columns used for stratification (mandatory + optional)"""
        
        # Mandatory columns
        mandatory_cols = [
            self.entity_col_var.get(),
            self.region_col_var.get(),
            self.product_col_var.get()
        ]
        
        # Optional additional columns
        optional_cols = []
        for col_var in [self.desk_col_var, self.book_col_var, self.trader_col_var]:
            col_val = col_var.get()
            if col_val != "None" and col_val in self.data.columns:
                optional_cols.append(col_val)
        
        return mandatory_cols, optional_cols
    
    def calculate_statistical_weights(self, data, column):
        """Calculate statistical weights based on exception frequency"""
        
        # Calculate frequency distribution
        counts = data[column].value_counts()
        total_count = len(data)
        frequencies = counts / total_count
        
        # Normalize to [0.1, 1.0] range to avoid zero weights
        if len(frequencies) > 1:
            min_freq = frequencies.min()
            max_freq = frequencies.max()
            if max_freq > min_freq:
                normalized_weights = 0.1 + 0.9 * (frequencies - min_freq) / (max_freq - min_freq)
            else:
                normalized_weights = pd.Series(0.5, index=frequencies.index)
        else:
            normalized_weights = pd.Series(0.5, index=frequencies.index)
        
        return normalized_weights.to_dict(), frequencies.to_dict(), counts.to_dict()
    
    def calculate_statistical_risk_scores(self):
        """Calculate risk scores statistically from exception data - Enhanced with additional columns"""
        
        if self.data is None:
            messagebox.showerror("Error", "Please load data first")
            return
        
        try:
            entity_col = self.entity_col_var.get()
            region_col = self.region_col_var.get()
            product_col = self.product_col_var.get()
            reason_col = self.reason_col_var.get()
            
            # Validate required columns exist
            required_cols = [entity_col, region_col, product_col, reason_col]
            missing_cols = [col for col in required_cols if col not in self.data.columns]
            
            if missing_cols:
                messagebox.showerror("Error", f"Missing columns: {missing_cols}")
                return
            
            # Calculate statistical weights for mandatory categories
            self.entity_risk_scores, self.entity_frequencies, self.entity_counts = self.calculate_statistical_weights(self.data, entity_col)
            self.regional_risk_scores, self.regional_frequencies, self.regional_counts = self.calculate_statistical_weights(self.data, region_col)
            self.product_weights, self.product_frequencies, self.product_counts = self.calculate_statistical_weights(self.data, product_col)
            self.reason_code_weights, self.reason_frequencies, self.reason_counts = self.calculate_statistical_weights(self.data, reason_col)
            
            # Calculate weights for additional optional columns
            self.additional_risk_weights = {}
            _, optional_cols = self.get_stratification_columns()
            
            for col in optional_cols:
                weights, frequencies, counts = self.calculate_statistical_weights(self.data, col)
                self.additional_risk_weights[col] = {
                    'weights': weights,
                    'frequencies': frequencies,
                    'counts': counts
                }
            
            # Calculate composite risk score
            self.calculate_composite_risk_score()
            
            # Update UI displays
            self.update_risk_displays()
            self.risk_calc_label.config(text="Statistical risk scores calculated successfully")
            
            messagebox.showinfo("Success", f"Statistical risk scores calculated from exception frequency data. Additional columns: {len(optional_cols)}")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to calculate statistical risk scores: {str(e)}")
    
    def calculate_composite_risk_score(self):
        """Calculate composite risk score using statistical weights - Enhanced"""
        
        entity_col = self.entity_col_var.get()
        region_col = self.region_col_var.get()
        product_col = self.product_col_var.get()
        reason_col = self.reason_col_var.get()
        
        # Score mandatory columns
        self.data['entity_risk'] = self.data[entity_col].map(self.entity_risk_scores).fillna(0.5)
        self.data['region_risk'] = self.data[region_col].map(self.regional_risk_scores).fillna(0.5)
        self.data['product_risk'] = self.data[product_col].map(self.product_weights).fillna(0.5)
        self.data['reason_risk'] = self.data[reason_col].map(self.reason_code_weights).fillna(0.5)
        
        # Ensure all risk columns are numeric
        self.data = self.ensure_numeric_column(self.data, 'entity_risk')
        self.data = self.ensure_numeric_column(self.data, 'region_risk')
        self.data = self.ensure_numeric_column(self.data, 'product_risk')
        self.data = self.ensure_numeric_column(self.data, 'reason_risk')
        
        # Score additional optional columns
        _, optional_cols = self.get_stratification_columns()
        additional_risks = []
        
        for col in optional_cols:
            if col in self.additional_risk_weights:
                risk_col_name = f'{col}_risk'
                self.data[risk_col_name] = self.data[col].map(
                    self.additional_risk_weights[col]['weights']
                ).fillna(0.5)
                self.data = self.ensure_numeric_column(self.data, risk_col_name)
                additional_risks.append(risk_col_name)
        
        # Calculate composite risk score with dynamic weighting
        total_factors = 4 + len(additional_risks)  # 4 mandatory + additional
        weight_per_factor = 1.0 / total_factors
        
        # Calculate weighted composite risk score
        risk_components = [
            weight_per_factor * self.data['entity_risk'],
            weight_per_factor * self.data['region_risk'],
            weight_per_factor * self.data['product_risk'],
            weight_per_factor * self.data['reason_risk']
        ]
        
        # Add additional risk components
        for risk_col in additional_risks:
            risk_components.append(weight_per_factor * self.data[risk_col])
        
        self.data['risk_score'] = sum(risk_components)
        
        # Ensure risk scores are between 0.01 and 1.0
        self.data = self.ensure_numeric_column(self.data, 'risk_score')
        self.data['risk_score'] = np.clip(self.data['risk_score'], 0.01, 1.0)
        
        # Create enhanced stratum identifiers
        mandatory_cols, optional_cols = self.get_stratification_columns()
        all_stratum_cols = mandatory_cols + optional_cols
        
        self.data['stratum'] = self.data[all_stratum_cols].apply(
            lambda x: '_'.join(x.astype(str)), axis=1
        )
    
    def update_risk_displays(self):
        """Update the risk score display tables with statistical data - Enhanced"""
        
        # Clear existing items
        for tree in [self.entity_tree, self.region_tree, self.product_tree, self.reason_tree, self.additional_tree]:
            for item in tree.get_children():
                tree.delete(item)
        
        entity_col = self.entity_col_var.get()
        region_col = self.region_col_var.get()
        product_col = self.product_col_var.get()
        reason_col = self.reason_col_var.get()
        
        # Update entity risk scores
        if entity_col in self.data.columns and hasattr(self, 'entity_counts'):
            for entity in self.safe_sort_unique(self.data[entity_col]):
                count = self.entity_counts.get(entity, 0)
                freq = self.entity_frequencies.get(entity, 0) * 100
                risk_score = self.entity_risk_scores.get(entity, 0.5)
                self.entity_tree.insert("", "end", text=str(entity), 
                                      values=(f"{count:,}", f"{freq:.1f}%", f"{risk_score:.3f}"))
        
        # Update regional risk scores
        if region_col in self.data.columns and hasattr(self, 'regional_counts'):
            for region in self.safe_sort_unique(self.data[region_col]):
                count = self.regional_counts.get(region, 0)
                freq = self.regional_frequencies.get(region, 0) * 100
                risk_score = self.regional_risk_scores.get(region, 0.5)
                self.region_tree.insert("", "end", text=str(region), 
                                      values=(f"{count:,}", f"{freq:.1f}%", f"{risk_score:.3f}"))
        
        # Update product weights
        if product_col in self.data.columns and hasattr(self, 'product_counts'):
            for product in self.safe_sort_unique(self.data[product_col]):
                count = self.product_counts.get(product, 0)
                freq = self.product_frequencies.get(product, 0) * 100
                weight = self.product_weights.get(product, 0.5)
                self.product_tree.insert("", "end", text=str(product), 
                                       values=(f"{count:,}", f"{freq:.1f}%", f"{weight:.3f}"))
        
        # Update reason code weights
        if reason_col in self.data.columns and hasattr(self, 'reason_counts'):
            for reason in self.safe_sort_unique(self.data[reason_col]):
                count = self.reason_counts.get(reason, 0)
                freq = self.reason_frequencies.get(reason, 0) * 100
                weight = self.reason_code_weights.get(reason, 0.5)
                self.reason_tree.insert("", "end", text=str(reason), 
                                      values=(f"{count:,}", f"{freq:.1f}%", f"{weight:.3f}"))
        
        # Update additional columns
        for col, risk_data in self.additional_risk_weights.items():
            parent = self.additional_tree.insert("", "end", text=f"{col.upper()}", values=("", "", ""))
            for item in self.safe_sort_unique(pd.Series(list(risk_data['weights'].keys()))):
                count = risk_data['counts'].get(item, 0)
                freq = risk_data['frequencies'].get(item, 0) * 100
                weight = risk_data['weights'].get(item, 0.5)
                self.additional_tree.insert(parent, "end", text=f"  {item}", 
                                          values=(f"{count:,}", f"{freq:.1f}%", f"{weight:.3f}"))
    
    def generate_sample_data(self):
        """Generate realistic OMRC exception data for testing - Enhanced with additional columns"""
        
        try:
            # Set random seed for reproducibility
            np.random.seed(42)
            n_records = 5000
            
            # Define entities and their regions
            entity_regions = {
                'HBAP': ['LN', 'AU', 'IN', 'PA', 'HK', 'SG'],
                'HBEU': ['LN', 'PA', 'FR', 'DE', 'IT', 'CH'],
                'HBUS': ['NY', 'CA', 'TX', 'IL', 'FL']
            }
            
            products = ['Cash_Bonds', 'Equities', 'IRD', 'FX_Derivatives', 
                       'ABS_MBS', 'Structured_Products', 'Repo', 'Commodities']
            
            reason_codes = ['Price_Mismatch', 'Model_Error', 'Data_Quality', 'Process_Delay',
                           'System_Error', 'Manual_Override', 'Counterparty_Issue', 'Settlement_Delay']
            
            desks = [f"DESK_{i:02d}" for i in range(1, 21)]
            books = [f"BOOK_{i:03d}" for i in range(1, 31)]
            traders = [f"TRADER_{i:03d}" for i in range(1, 101)]
            
            # Generate base data with realistic distributions
            entities = []
            regions = []
            
            # Create skewed distribution (more exceptions in certain entities/regions)
            entity_probs = {'HBAP': 0.45, 'HBEU': 0.35, 'HBUS': 0.20}
            
            for _ in range(n_records):
                entity = np.random.choice(list(entity_regions.keys()), p=list(entity_probs.values()))
                region = np.random.choice(entity_regions[entity])
                entities.append(entity)
                regions.append(region)
            
            data = {
                'exception_id': range(1, n_records + 1),
                'legal_entity': entities,
                'region': regions,
                'product_type': np.random.choice(products, n_records, 
                                               p=[0.25, 0.20, 0.15, 0.12, 0.08, 0.06, 0.08, 0.06]),
                'reason_code': np.random.choice(reason_codes, n_records,
                                              p=[0.25, 0.15, 0.15, 0.10, 0.10, 0.08, 0.10, 0.07]),
                'desk_id': np.random.choice(desks, n_records),
                'book_id': np.random.choice(books, n_records),
                'trader_id': np.random.choice(traders, n_records),
                'trade_value': np.random.lognormal(15, 1.5, n_records),
                'aging_days': np.random.exponential(8, n_records).astype(int),
                'l1_closure': np.random.choice([1, 0], n_records, p=[0.85, 0.15]),
                'counterparty_rating': np.random.choice(['A', 'B', 'C', 'D'], n_records, p=[0.4, 0.3, 0.2, 0.1]),
                'business_date': pd.date_range(start='2024-01-01', periods=n_records, freq='D').strftime('%Y-%m-%d')
            }
            
            df = pd.DataFrame(data)
            
            # Filter to L1 exceptions only
            df = df[df['l1_closure'] == 1].reset_index(drop=True)
            
            self.data = df
            self.update_column_dropdowns()
            self.update_data_preview()
            self.data_label.config(text=f"Generated {len(df):,} L1 exception records")
            
            messagebox.showinfo("Success", f"Generated {len(df):,} L1 exception records with enhanced columns")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to generate sample data: {str(e)}")
    
    def update_data_preview(self):
        """Update the data preview treeview with horizontal scrolling"""
        
        if self.data is None:
            return
            
        # Clear existing data
        for item in self.tree.get_children():
            self.tree.delete(item)
        
        # Configure columns (show all columns for horizontal scrolling)
        display_cols = list(self.data.columns)
        
        self.tree["columns"] = display_cols
        self.tree["show"] = "headings"
        
        # Configure column headings and widths
        for col in display_cols:
            self.tree.heading(col, text=col.replace('_', ' ').title())
            self.tree.column(col, width=120, minwidth=80)
        
        # Add data (first 100 rows for performance)
        for _, row in self.data.head(100).iterrows():
            values = [str(row.get(col, '')) for col in display_cols]
            self.tree.insert("", "end", values=values)
    
    def calculate_sample_size(self, stratum_data, confidence=95, margin=0.05, p_stratum=None):
        """Calculate enhanced sample size using statistical risk weights with enhanced risk weighting - FIXED"""
        
        if len(stratum_data) == 0:
            return 0, {}
        
        # Ensure numeric data types
        stratum_data = self.ensure_numeric_column(stratum_data.copy(), 'risk_score')
        
        # Safe parameter conversion
        confidence = self.safe_float_conversion(confidence, 95)
        margin = self.safe_float_conversion(margin, 0.05)
        if p_stratum is not None:
            p_stratum = self.safe_float_conversion(p_stratum, 0.15)
            
        # Z-scores for confidence levels
        z_scores = {90: 1.645, 95: 1.96, 99: 2.576}
        z = z_scores.get(confidence, 1.96)
        
        # Calculate p_stratum if not provided
        if p_stratum is None:
            high_risk_count = len(stratum_data[stratum_data['risk_score'] > 0.7])
            p_stratum = high_risk_count / len(stratum_data) if len(stratum_data) > 0 else 0.01
            p_stratum = max(p_stratum, 0.01)
        
        q_stratum = 1 - p_stratum
        
        # Base statistical sample size
        if margin > 0:
            base_n = (z**2 * p_stratum * q_stratum) / (margin**2)
        else:
            base_n = 100
        
        # Enhanced risk weighting with additional columns
        mandatory_cols, optional_cols = self.get_stratification_columns()
        
        # Calculate risk weight from all available dimensions
        risk_weights = []
        
        # Mandatory dimensions
        if len(stratum_data) > 0:
            entity = stratum_data[mandatory_cols[0]].iloc[0] if mandatory_cols[0] in stratum_data.columns else None
            region = stratum_data[mandatory_cols[1]].iloc[0] if mandatory_cols[1] in stratum_data.columns else None
            product = stratum_data[mandatory_cols[2]].iloc[0] if mandatory_cols[2] in stratum_data.columns else None
            
            if entity:
                entity_risk = self.entity_risk_scores.get(entity, 0.5)
                risk_weights.append(entity_risk)
            if region:
                region_risk = self.regional_risk_scores.get(region, 0.5)
                risk_weights.append(region_risk)
            if product:
                product_risk = self.product_weights.get(product, 0.5)
                risk_weights.append(product_risk)
            
            # Optional dimensions
            for col in optional_cols:
                if col in stratum_data.columns and col in self.additional_risk_weights:
                    item_value = stratum_data[col].iloc[0]
                    item_risk = self.additional_risk_weights[col]['weights'].get(item_value, 0.5)
                    risk_weights.append(item_risk)
        
        # Calculate composite risk weight
        if risk_weights:
            avg_risk = sum(risk_weights) / len(risk_weights)
            risk_weight = 1.0 + avg_risk  # Convert to 1.0-2.0 multiplier
        else:
            risk_weight = 1.2
        
        # Apply risk weighting
        adjusted_n = base_n * risk_weight
        
        # Apply constraints
        n_min = 5 if len(stratum_data) > 5 else len(stratum_data)
        n_max = len(stratum_data)
        
        final_n = max(math.ceil(adjusted_n), n_min)
        final_n = min(final_n, n_max)
        
        # Return calculation details
        details = {
            'population': len(stratum_data),
            'risk_weight': risk_weight,
            'base_sample': math.ceil(base_n),
            'risk_adjusted': math.ceil(adjusted_n),
            'final_sample': final_n,
            'dimensions_used': len(risk_weights)
        }
        
        return final_n, details
    
    def traditional_sampling(self, data, total_sample_size):
        """Traditional random sampling - FIXED"""
        
        # Safe parameter conversion
        total_sample_size = self.safe_int_conversion(total_sample_size, 100)
        
        if total_sample_size >= len(data):
            return data
            
        return data.sample(n=total_sample_size, random_state=42)
    
    def risk_based_sampling(self, data, total_sample_size):
        """Enhanced statistical risk-based stratified sampling with multi-dimensional stratification - FIXED"""
        
        # Safe parameter conversion and data preparation
        total_sample_size = self.safe_int_conversion(total_sample_size, 100)
        data = self.ensure_numeric_column(data.copy(), 'risk_score')
        
        samples = []
        self.last_stratum_samples = {}
        self.stratum_details = {}
        
        # Get stratification columns
        mandatory_cols, optional_cols = self.get_stratification_columns()
        all_stratum_cols = mandatory_cols + optional_cols
        
        if not all_stratum_cols or not all(col in data.columns for col in all_stratum_cols):
            return self.traditional_sampling(data, total_sample_size)
        
        strata = data.groupby(all_stratum_cols)
        
        # Calculate sample size for each stratum
        total_calculated = 0
        
        for name, group in strata:
            stratum_size, details = self.calculate_sample_size(group)
            self.last_stratum_samples[name] = min(stratum_size, len(group))
            self.stratum_details[name] = details
            total_calculated += self.last_stratum_samples[name]
        
        # Adjust if total calculated exceeds target
        if total_calculated > total_sample_size and total_calculated > 0:
            adjustment_factor = total_sample_size / total_calculated
            for name in self.last_stratum_samples:
                self.last_stratum_samples[name] = max(1, int(self.last_stratum_samples[name] * adjustment_factor))
                self.stratum_details[name]['final_sample'] = self.last_stratum_samples[name]
        
        # Select samples from each stratum
        for name, group in strata:
            sample_size = self.last_stratum_samples.get(name, 0)
            if sample_size > 0:
                # Enhanced sampling within stratum
                group = self.ensure_numeric_column(group.copy(), 'risk_score')
                group_sorted = group.sort_values('risk_score', ascending=False)
                
                # Take top risk items first (50% of allocation or available high-risk items)
                high_risk_threshold = 0.7
                high_risk_data = group_sorted[group_sorted['risk_score'] > high_risk_threshold]
                high_risk_count = min(sample_size // 2, len(high_risk_data))
                
                if high_risk_count > 0:
                    high_risk_sample = high_risk_data.head(high_risk_count)
                else:
                    high_risk_sample = pd.DataFrame()
                
                # Fill remaining quota with stratified random selection
                remaining_needed = sample_size - len(high_risk_sample)
                if remaining_needed > 0:
                    remaining_data = group[~group.index.isin(high_risk_sample.index)]
                    if len(remaining_data) > 0:
                        remaining_sample = remaining_data.sample(n=min(remaining_needed, len(remaining_data)), random_state=42)
                        stratum_sample = pd.concat([high_risk_sample, remaining_sample])
                    else:
                        stratum_sample = high_risk_sample
                else:
                    stratum_sample = high_risk_sample
                
                if len(stratum_sample) > 0:
                    samples.append(stratum_sample)
        
        return pd.concat(samples) if samples else data.sample(n=min(total_sample_size, len(data)), random_state=42)
    
    def detect_anomalies(self, data, contamination=0.1):
        """Enhanced anomaly detection using Isolation Forest with multi-dimensional features - FIXED"""
        
        try:
            # Safe parameter conversion
            contamination = self.safe_float_conversion(contamination, 0.1)
            contamination = max(0.01, min(0.5, contamination))  # Constrain to valid range
            
            # Enhanced feature set including all risk dimensions
            feature_cols = ['risk_score', 'entity_risk', 'region_risk', 'product_risk', 'reason_risk']
            
            # Add additional risk columns if available
            _, optional_cols = self.get_stratification_columns()
            for col in optional_cols:
                risk_col = f'{col}_risk'
                if risk_col in data.columns:
                    feature_cols.append(risk_col)
            
            # Get available features and ensure they are numeric
            available_features = []
            for col in feature_cols:
                if col in data.columns:
                    data = self.ensure_numeric_column(data.copy(), col)
                    available_features.append(col)
            
            if len(available_features) < 2:
                # Fallback: return highest risk scores
                data = self.ensure_numeric_column(data.copy(), 'risk_score')
                return data.nlargest(int(len(data) * contamination), 'risk_score')
            
            features = data[available_features].fillna(0.5)
            
            # Standardize features
            scaler = StandardScaler()
            scaled_features = scaler.fit_transform(features)
            
            # Apply Isolation Forest
            iso_forest = IsolationForest(contamination=contamination, random_state=42, n_estimators=200)
            anomaly_labels = iso_forest.fit_predict(scaled_features)
            
            # Get anomalies
            anomaly_indices = data.index[anomaly_labels == -1]
            return data.loc[anomaly_indices]
            
        except Exception as e:
            print(f"Anomaly detection failed: {str(e)}")
            # Fallback: return highest risk scores
            data = self.ensure_numeric_column(data.copy(), 'risk_score')
            return data.nlargest(int(len(data) * contamination), 'risk_score')
    
    def hybrid_sampling(self, data, total_sample_size):
        """Enhanced hybrid sampling with multi-dimensional stratification - FIXED"""
        
        # Safe parameter conversion
        total_sample_size = self.safe_int_conversion(total_sample_size, 100)
        
        # Dynamic allocation based on data characteristics
        risk_based_size = int(total_sample_size * 0.65)  # Increased for better stratification
        anomaly_size = int(total_sample_size * 0.25)     # Enhanced anomaly detection
        random_size = total_sample_size - risk_based_size - anomaly_size
        
        samples = []
        
        # 1. Enhanced risk-based sampling (65%)
        if risk_based_size > 0:
            risk_sample = self.risk_based_sampling(data, risk_based_size)
            samples.append(risk_sample)
        
        # 2. Enhanced anomaly detection (25%)
        if anomaly_size > 0:
            try:
                contamination = self.safe_float_conversion(self.anomaly_var.get(), 0.1)
            except:
                contamination = 0.1
            
            anomaly_sample = self.detect_anomalies(data, contamination)
            if len(anomaly_sample) > anomaly_size:
                anomaly_sample = anomaly_sample.sample(n=anomaly_size, random_state=42)
            samples.append(anomaly_sample)
        
        # 3. Strategic random (10%)
        if random_size > 0:
            used_indices = pd.concat(samples).index if samples else pd.Index([])
            remaining_data = data[~data.index.isin(used_indices)]
            
            if len(remaining_data) > 0:
                random_sample = remaining_data.sample(n=min(random_size, len(remaining_data)), random_state=42)
                samples.append(random_sample)
        
        # Combine and optimize
        if samples:
            final_sample = pd.concat(samples).drop_duplicates()
            
            # Fill any remaining quota
            if len(final_sample) < total_sample_size:
                remaining_data = data[~data.index.isin(final_sample.index)]
                if len(remaining_data) > 0:
                    additional_needed = total_sample_size - len(final_sample)
                    additional_sample = remaining_data.sample(n=min(additional_needed, len(remaining_data)), random_state=42)
                    final_sample = pd.concat([final_sample, additional_sample])
        else:
            final_sample = data.sample(n=min(total_sample_size, len(data)), random_state=42)
        
        return final_sample
    
    def generate_comparison_samples(self):
        """Generate samples using different methods with enhanced tracking and analysis - FIXED"""
        
        if self.data is None:
            messagebox.showerror("Error", "Please load or generate data first")
            return
            
        if 'risk_score' not in self.data.columns:
            messagebox.showerror("Error", "Please calculate risk scores first")
            return
        
        try:
            # FIXED: Safe parameter conversion from Tkinter inputs
            confidence = self.safe_float_conversion(self.confidence_var.get(), 95)
            margin = self.safe_float_conversion(self.margin_var.get(), 0.05)
            p = self.safe_float_conversion(self.risk_var.get(), 0.15)
            
            # Validate inputs
            if margin <= 0 or margin >= 1:
                margin = 0.05
                self.margin_var.set("0.05")
                
            if p <= 0 or p >= 1:
                p = 0.15
                self.risk_var.set("0.15")
            
            # Traditional sample size calculation
            z_scores = {90: 1.645, 95: 1.96, 99: 2.576}
            z = z_scores.get(confidence, 1.96)
            q = 1 - p
            n = (z**2 * p * q) / (margin**2)
            
            # Apply finite population correction
            N = len(self.data)
            if N > 0:
                n_adjusted = n / (1 + (n - 1) / N)
            else:
                n_adjusted = 0
                
            target_sample_size = max(1, math.ceil(n_adjusted))
            
            # Enhanced sampling parameters
            mandatory_cols, optional_cols = self.get_stratification_columns()
            
            self.sampling_parameters = {
                'confidence': confidence,
                'margin': margin,
                'p': p,
                'q': q,
                'z': z,
                'base_n': math.ceil(n),
                'adjusted_n': target_sample_size,
                'population_N': N,
                'formula_str': f"n = (z^2 * p * q) / margin^2 = ({z:.3f}^2 * {p} * {q:.3f}) / {margin}^2 = {math.ceil(n)}",
                'stratification_dimensions': len(mandatory_cols) + len(optional_cols),
                'mandatory_cols': mandatory_cols,
                'optional_cols': optional_cols
            }
            
            # Ensure risk_score column is numeric
            self.data = self.ensure_numeric_column(self.data, 'risk_score')
            
            # Clear previous results
            self.clear_results()
            
            # Generate samples
            results = {}
            
            self.log_results("=" * 90)
            self.log_results("OMRC ENHANCED MULTI-DIMENSIONAL STATISTICAL RISK-BASED SAMPLING ANALYSIS")
            self.log_results("=" * 90)
            self.log_results(f"Population Size (N): {N:,}")
            self.log_results(f"Target Sample Size (n): {target_sample_size:,}")
            self.log_results(f"Stratification Dimensions: {len(mandatory_cols) + len(optional_cols)}")
            self.log_results(f"  - Mandatory: {mandatory_cols}")
            self.log_results(f"  - Optional: {optional_cols}")
            self.log_results(f"Formula Used: {self.sampling_parameters['formula_str']}")
            self.log_results("")
            
            if self.method_vars['traditional'].get():
                self.log_results("-" * 60)
                self.log_results("TRADITIONAL RANDOM SAMPLING")
                self.log_results("-" * 60)
                traditional_sample = self.traditional_sampling(self.data, target_sample_size)
                results['traditional'] = traditional_sample
                self.analyze_sample('Traditional Random', traditional_sample, self.data)
            
            if self.method_vars['risk_based'].get():
                self.log_results("-" * 60)
                self.log_results("ENHANCED STATISTICAL RISK-BASED STRATIFIED SAMPLING")
                self.log_results("-" * 60)
                risk_sample = self.risk_based_sampling(self.data, target_sample_size)
                results['risk_based'] = risk_sample
                self.analyze_sample('Enhanced Risk-Based', risk_sample, self.data)
            
            if self.method_vars['hybrid'].get():
                self.log_results("-" * 60)
                self.log_results("ENHANCED HYBRID MULTI-DIMENSIONAL SAMPLING")
                self.log_results("-" * 60)
                hybrid_sample = self.hybrid_sampling(self.data, target_sample_size)
                results['hybrid'] = hybrid_sample
                self.analyze_sample('Enhanced Hybrid', hybrid_sample, self.data)
            
            # Store results
            self.comparison_results = results
            
            # Generate enhanced analysis
            self.generate_enhanced_comparison_summary(results, self.data)
            self.update_enhanced_summary_table(results, self.data)
            self.update_analysis_tabs(results, self.data)
            
            messagebox.showinfo("Success", f"Enhanced multi-dimensional sampling comparison completed! Dimensions: {len(mandatory_cols) + len(optional_cols)}")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to generate comparison samples: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def clear_results(self):
        """Clear all result displays"""
        self.results_text.delete(1.0, tk.END)
        for item in self.summary_tree.get_children():
            self.summary_tree.delete(item)
        for item in self.stratum_tree.get_children():
            self.stratum_tree.delete(item)
    
    def analyze_sample(self, method_name, sample, population):
        """Enhanced sample analysis with coverage metrics - FIXED"""
        
        if len(sample) == 0:
            self.log_results(f"No samples generated for {method_name}")
            return
        
        try:
            # Ensure numeric columns
            sample = self.ensure_numeric_column(sample.copy(), 'risk_score')
            population = self.ensure_numeric_column(population.copy(), 'risk_score')
            
            mandatory_cols, _ = self.get_stratification_columns()
            entity_col, region_col, product_col = mandatory_cols[:3]
            
            # Basic statistics
            self.log_results(f"Sample Size: {len(sample):,}")
            self.log_results(f"Average Risk Score: {sample['risk_score'].mean():.4f}")
            self.log_results(f"Risk Score Range: {sample['risk_score'].min():.4f} - {sample['risk_score'].max():.4f}")
            self.log_results(f"High-Risk Count (>0.7): {len(sample[sample['risk_score'] > 0.7]):,}")
            
            # Enhanced coverage analysis
            entity_coverage = len(sample[entity_col].unique()) if entity_col in sample.columns else 0
            region_coverage = len(sample[region_col].unique()) if region_col in sample.columns else 0
            product_coverage = len(sample[product_col].unique()) if product_col in sample.columns else 0
            
            total_entities = len(population[entity_col].unique()) if entity_col in population.columns else 0
            total_regions = len(population[region_col].unique()) if region_col in population.columns else 0
            total_products = len(population[product_col].unique()) if product_col in population.columns else 0
            
            if total_entities > 0:
                self.log_results(f"Entity Coverage: {entity_coverage}/{total_entities} ({entity_coverage/total_entities*100:.1f}%)")
            if total_regions > 0:
                self.log_results(f"Region Coverage: {region_coverage}/{total_regions} ({region_coverage/total_regions*100:.1f}%)")
            if total_products > 0:
                self.log_results(f"Product Coverage: {product_coverage}/{total_products} ({product_coverage/total_products*100:.1f}%)")
            
            # High-risk coverage
            total_high_risk = len(population[population['risk_score'] > 0.7])
            sample_high_risk = len(sample[sample['risk_score'] > 0.7])
            
            if total_high_risk > 0:
                coverage = (sample_high_risk / total_high_risk) * 100
            else:
                coverage = 0
                
            self.log_results(f"High-Risk Coverage: {coverage:.2f}% ({sample_high_risk:,}/{total_high_risk:,})")
            self.log_results("")
            
        except Exception as e:
            self.log_results(f"Error analyzing {method_name}: {str(e)}")
    
    def generate_enhanced_comparison_summary(self, results, population):
        """Generate enhanced summary with multi-dimensional analysis - FIXED"""
        
        try:
            # Ensure numeric columns
            population = self.ensure_numeric_column(population.copy(), 'risk_score')
            
            self.log_results("=" * 90)
            self.log_results("ENHANCED COMPARISON SUMMARY")
            self.log_results("=" * 90)
            
            # Enhanced metrics
            mandatory_cols, _ = self.get_stratification_columns()
            entity_col, region_col, product_col = mandatory_cols[:3]
            total_high_risk = len(population[population['risk_score'] > 0.7])
            
            self.log_results("Enhanced Method Performance Summary:")
            self.log_results(f"{'Method':<30} {'Sample':<8} {'High-Risk':<10} {'Coverage':<10} {'Avg Risk':<10} {'Entities':<9} {'Regions':<8}")
            self.log_results("-" * 90)
            
            for method_name, sample in results.items():
                if len(sample) > 0:
                    sample = self.ensure_numeric_column(sample.copy(), 'risk_score')
                    
                    high_risk_count = len(sample[sample['risk_score'] > 0.7])
                    coverage = (high_risk_count / total_high_risk * 100) if total_high_risk > 0 else 0
                    avg_risk = sample['risk_score'].mean()
                    
                    entities = len(sample[entity_col].unique()) if entity_col in sample.columns else 0
                    regions = len(sample[region_col].unique()) if region_col in sample.columns else 0
                    
                    method_display = method_name.replace('_', ' ').title()
                    self.log_results(f"{method_display:<30} {len(sample):<8,} {high_risk_count:<10,} {coverage:<10.1f} {avg_risk:<10.4f} {entities:<9} {regions:<8}")
            
            self.log_results("")
            
        except Exception as e:
            self.log_results(f"Error generating comparison summary: {str(e)}")
    
    def update_enhanced_summary_table(self, results, population):
        """Update summary table with enhanced metrics - FIXED"""
        
        try:
            # Clear existing items
            for item in self.summary_tree.get_children():
                self.summary_tree.delete(item)
            
            # Ensure numeric columns
            population = self.ensure_numeric_column(population.copy(), 'risk_score')
            
            mandatory_cols, _ = self.get_stratification_columns()
            entity_col, region_col, product_col = mandatory_cols[:3]
            total_high_risk = len(population[population['risk_score'] > 0.7])
            
            for method_name, sample in results.items():
                if len(sample) > 0:
                    sample = self.ensure_numeric_column(sample.copy(), 'risk_score')
                    
                    high_risk_count = len(sample[sample['risk_score'] > 0.7])
                    coverage = (high_risk_count / total_high_risk * 100) if total_high_risk > 0 else 0
                    avg_risk = sample['risk_score'].mean()
                    
                    entities = len(sample[entity_col].unique()) if entity_col in sample.columns else 0
                    regions = len(sample[region_col].unique()) if region_col in sample.columns else 0
                    
                    method_display = method_name.replace('_', ' ').title()
                    
                    self.summary_tree.insert("", "end", text=method_display, values=(
                        f"{len(sample):,}",
                        f"{high_risk_count:,}",
                        f"{coverage:.1f}%",
                        f"{avg_risk:.4f}",
                        f"{entities}",
                        f"{regions}"
                    ))
                    
        except Exception as e:
            messagebox.showerror("Error", f"Failed to update summary table: {str(e)}")
    
    def update_analysis_tabs(self, results, population):
        """Update all analysis tabs with enhanced multi-dimensional data"""
        
        # Executive Summary
        self.update_executive_summary()
        
        # Enhanced Stratum Analysis
        self.update_stratum_analysis()
        
        # Detailed Analysis
        self.update_detailed_analysis(results, population)
        
        # Enhanced Risk Breakdown
        self.update_risk_breakdown(population)
    
    def update_executive_summary(self):
        """Update executive summary with enhanced multi-dimensional analysis"""
        
        self.exec_text.delete(1.0, tk.END)
        
        if not hasattr(self, 'sampling_parameters'):
            self.exec_text.insert(1.0, "No sampling analysis performed yet.")
            return
        
        params = self.sampling_parameters
        
        summary = f"""=== OMRC ENHANCED MULTI-DIMENSIONAL SAMPLING EXECUTIVE SUMMARY ===
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

POPULATION AND SAMPLE SIZE CALCULATION:
Population size (N): {params['population_N']:,}
Target Sample size (n): {params['adjusted_n']:,}

ENHANCED STRATIFICATION STRUCTURE:
Total Dimensions: {params['stratification_dimensions']}
Mandatory Dimensions (3): {params['mandatory_cols']}
Optional Dimensions ({len(params['optional_cols'])}): {params['optional_cols']}

SAMPLING FORMULA USED:
Base Formula: n = (z  p  q) / margin
Where:
  z = {params['z']:.3f} (for {params['confidence']}% confidence level)
  p = {params['p']} (risk appetite/expected error rate)
  q = {params['q']:.3f} (1 - p)
  margin = {self.margin_var.get()} (margin of error)

Calculation Steps:
1. Base sample size = ({params['z']:.3f}  {params['p']}  {params['q']:.3f}) / {self.margin_var.get()} = {params['base_n']:,}
2. Finite population correction applied
3. Final adjusted sample size = {params['adjusted_n']:,}

ENHANCED STATISTICAL WEIGHT NORMALIZATION:
Risk Weight = 0.1 + 0.9  (Frequency - Min_Freq) / (Max_Freq - Min_Freq)
- Multi-dimensional risk weighting with {params['stratification_dimensions']} dimensions
- Dynamic weight allocation: 1/{params['stratification_dimensions']} per dimension
- Composite risk score = (dimension_weight  dimension_risk)

RISK-BASED SAMPLE SIZE ALLOCATION:
For each stratum (Multi-dimensional: {'  '.join(params['mandatory_cols'] + params['optional_cols'])}):
1. Calculate base sample size using statistical formula
2. Apply composite risk weight: w_risk = 1.0 + avg(all_dimension_risks)
3. Adjusted sample = base_sample  w_risk
4. Apply constraints: min(5), max(stratum_size)

"""
        
        if hasattr(self, 'last_stratum_samples') and self.last_stratum_samples:
            summary += f"\nENHANCED STRATUM SAMPLE ALLOCATION SUMMARY:\n"
            summary += f"{'Multi-Dimensional Stratum':<50} {'Population':>10} {'Dimensions':>10} {'Allocated':>10}\n"
            summary += "-" * 82 + "\n"
            
            total_pop = 0
            total_allocated = 0
            
            for stratum_name, allocated in list(self.last_stratum_samples.items())[:20]:  # Show first 20
                if hasattr(self, 'stratum_details') and stratum_name in self.stratum_details:
                    details = self.stratum_details[stratum_name]
                    pop = details['population']
                    dims = details.get('dimensions_used', len(params['mandatory_cols']) + len(params['optional_cols']))
                    
                    total_pop += pop
                    total_allocated += allocated
                    
                    stratum_str = str(stratum_name)[:48] + ".." if len(str(stratum_name)) > 50 else str(stratum_name)
                    summary += f"{stratum_str:<50} {pop:>10,} {dims:>10} {allocated:>10,}\n"
            
            if len(self.last_stratum_samples) > 20:
                summary += f"... and {len(self.last_stratum_samples) - 20} more strata\n"
            
            summary += "-" * 82 + "\n"
            summary += f"{'TOTAL (All Strata)':<50} {total_pop:>10,} {'':<10} {total_allocated:>10,}\n"
        
        self.exec_text.insert(1.0, summary)
    
    def update_stratum_analysis(self):
        """Update stratum analysis with enhanced multi-dimensional breakdown"""
        
        # Clear existing items
        for item in self.stratum_tree.get_children():
            self.stratum_tree.delete(item)
        
        if not hasattr(self, 'stratum_details') or not self.stratum_details:
            return
        
        # Sort strata by population size (descending)
        sorted_strata = sorted(self.stratum_details.items(), 
                              key=lambda x: x[1]['population'], reverse=True)
        
        for stratum_name, details in sorted_strata[:100]:  # Show top 100 strata
            stratum_str = str(stratum_name)
            
            self.stratum_tree.insert("", "end", text=stratum_str, values=(
                f"{details['population']:,}",
                f"{details['risk_weight']:.3f}",
                f"{details['base_sample']:,}",
                f"{details['risk_adjusted']:,}",
                f"{details['final_sample']:,}"
            ))
    
    def update_detailed_analysis(self, results, population):
        """Update detailed technical analysis"""
        
        analysis_content = self.results_text.get(1.0, tk.END)
        self.analysis_text.delete(1.0, tk.END)
        self.analysis_text.insert(1.0, analysis_content)
    
    def update_risk_breakdown(self, population):
        """Update risk breakdown with enhanced multi-dimensional analysis - COMPLETELY FIXED"""
        
        try:
            # Clear existing items
            for item in self.risk_breakdown_tree.get_children():
                self.risk_breakdown_tree.delete(item)
            
            # Ensure numeric columns
            population = self.ensure_numeric_column(population.copy(), 'risk_score')
            
            mandatory_cols, optional_cols = self.get_stratification_columns()
            
            # Enhanced breakdown by all dimensions
            all_dimensions = mandatory_cols + optional_cols
            
            for i, col in enumerate(all_dimensions):
                if col in population.columns:
                    # FIXED: Safe dimension type determination
                    if i < len(mandatory_cols):
                        dimension_type = "MANDATORY"
                    else:
                        dimension_type = "OPTIONAL"
                    
                    parent = self.risk_breakdown_tree.insert("", "end", text=f"{col.upper()} ({dimension_type})", values=("", "", "", ""))
                    
                    # Get risk scores for this dimension
                    if col == mandatory_cols[0]:  # Entity
                        risk_scores = self.entity_risk_scores
                    elif len(mandatory_cols) > 1 and col == mandatory_cols[1]:  # Region
                        risk_scores = self.regional_risk_scores
                    elif len(mandatory_cols) > 2 and col == mandatory_cols[2]:  # Product
                        risk_scores = self.product_weights
                    elif col in self.additional_risk_weights:
                        risk_scores = self.additional_risk_weights[col]['weights']
                    else:
                        risk_scores = {}
                    
                    # FIXED: Safe sorting and iteration of unique values
                    unique_items = self.safe_sort_unique(population[col])
                    
                    for item in unique_items:
                        try:
                            item_data = population[population[col] == item].copy()
                            # Ensure numeric before comparison
                            item_data = self.ensure_numeric_column(item_data, 'risk_score')
                            
                            high_risk_count = len(item_data[item_data['risk_score'] > 0.7])
                            high_risk_pct = (high_risk_count / len(item_data) * 100) if len(item_data) > 0 else 0
                            avg_risk = item_data['risk_score'].mean()
                            
                            self.risk_breakdown_tree.insert(parent, "end", text=f"  {item}", values=(
                                f"{len(item_data):,}",
                                f"{high_risk_count:,}",
                                f"{high_risk_pct:.1f}%",
                                f"{avg_risk:.4f}"
                            ))
                        except Exception as item_error:
                            print(f"Error processing item {item} in column {col}: {str(item_error)}")
                            continue
                            
        except Exception as e:
            print(f"Error updating risk breakdown: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def update_visualizations(self):
        """Enhanced visualization suite with multi-dimensional analysis"""
        
        if not self.comparison_results or self.data is None:
            return
        
        # Clear existing plots
        for ax in self.axes.flat:
            ax.clear()
        
        chart_type = self.chart_type.get()
        
        if chart_type == "comprehensive":
            self.create_comprehensive_charts()
        elif chart_type == "coverage":
            self.create_coverage_charts()
        elif chart_type == "distribution":
            self.create_distribution_charts()
        elif chart_type == "risk_analysis":
            self.create_risk_analysis_charts()
        elif chart_type == "stratum_heatmap":
            self.create_stratum_heatmap()
        
        self.fig.tight_layout(pad=3.0)
        self.canvas.draw()
    
    def create_comprehensive_charts(self):
        """Create comprehensive chart suite - FIXED"""
        
        try:
            # Ensure numeric columns
            self.data = self.ensure_numeric_column(self.data, 'risk_score')
            
            # Chart 1: Enhanced Coverage Comparison
            methods = []
            coverages = []
            
            total_high_risk = len(self.data[self.data['risk_score'] > 0.7])
            
            for method_name, sample in self.comparison_results.items():
                if len(sample) > 0:
                    sample = self.ensure_numeric_column(sample.copy(), 'risk_score')
                    high_risk_count = len(sample[sample['risk_score'] > 0.7])
                    coverage = (high_risk_count / total_high_risk * 100) if total_high_risk > 0 else 0
                    methods.append(method_name.replace('_', ' ').title())
                    coverages.append(coverage)
            
            if methods and coverages:
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
                bars = self.axes[0, 0].bar(methods, coverages, color=colors[:len(methods)])
                self.axes[0, 0].set_title('High-Risk Coverage Comparison\n(Enhanced Multi-Dimensional)', fontweight='bold', fontsize=12)
                self.axes[0, 0].set_ylabel('Coverage %')
                self.axes[0, 0].tick_params(axis='x', rotation=45)
                
                # Enhanced value labels
                for bar, coverage in zip(bars, coverages):
                    height = bar.get_height()
                    self.axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 1,
                                       f'{coverage:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)
            
            # Chart 2: Multi-Dimensional Entity Risk Heatmap
            if hasattr(self, 'entity_risk_scores') and len(self.entity_risk_scores) > 1:
                entities = list(self.entity_risk_scores.keys())
                risk_scores = list(self.entity_risk_scores.values())
                
                # Create color mapping
                colors = plt.cm.RdYlBu_r(np.linspace(0, 1, len(entities)))
                bars = self.axes[0, 1].barh(entities, risk_scores, color=colors)
                self.axes[0, 1].set_title('Entity Risk Scores\n(Statistical Frequency-Based)', fontweight='bold', fontsize=12)
                self.axes[0, 1].set_xlabel('Risk Score')
                
                # Add value labels
                for bar, score in zip(bars, risk_scores):
                    width = bar.get_width()
                    self.axes[0, 1].text(width + 0.01, bar.get_y() + bar.get_height()/2.,
                                       f'{score:.3f}', ha='left', va='center', fontsize=9)
            
            # Chart 3: Risk Score Distribution Comparison
            for i, (method_name, sample) in enumerate(self.comparison_results.items()):
                if len(sample) > 0:
                    sample = self.ensure_numeric_column(sample.copy(), 'risk_score')
                    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
                    self.axes[0, 2].hist(sample['risk_score'], bins=25, alpha=0.6, 
                                       label=method_name.replace('_', ' ').title(), 
                                       color=colors[i % 3], edgecolor='black', linewidth=0.5)
            
            self.axes[0, 2].set_title('Risk Score Distribution\n(Multi-Dimensional Composite)', fontweight='bold', fontsize=12)
            self.axes[0, 2].set_xlabel('Risk Score')
            self.axes[0, 2].set_ylabel('Frequency')
            self.axes[0, 2].legend()
            self.axes[0, 2].grid(True, alpha=0.3)
            
            # Chart 4: Sample Size by Method with Efficiency Metrics
            methods = []
            sample_sizes = []
            efficiency_scores = []
            
            for method_name, sample in self.comparison_results.items():
                if len(sample) > 0:
                    sample = self.ensure_numeric_column(sample.copy(), 'risk_score')
                    methods.append(method_name.replace('_', ' ').title())
                    sample_sizes.append(len(sample))
                    
                    # Calculate efficiency (high-risk coverage per sample)
                    high_risk_count = len(sample[sample['risk_score'] > 0.7])
                    efficiency = (high_risk_count / len(sample)) * 100 if len(sample) > 0 else 0
                    efficiency_scores.append(efficiency)
            
            if methods and sample_sizes:
                bars = self.axes[1, 0].bar(methods, sample_sizes, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
                self.axes[1, 0].set_title('Sample Sizes with Efficiency\n(High-Risk % per Sample)', fontweight='bold', fontsize=12)
                self.axes[1, 0].set_ylabel('Sample Size')
                self.axes[1, 0].tick_params(axis='x', rotation=45)
                
                # Add efficiency labels
                for bar, size, eff in zip(bars, sample_sizes, efficiency_scores):
                    height = bar.get_height()
                    self.axes[1, 0].text(bar.get_x() + bar.get_width()/2., height/2,
                                       f'{size:,}\n({eff:.1f}%)', ha='center', va='center', 
                                       fontweight='bold', fontsize=9, color='white')
            
            # Chart 5: Enhanced Regional Risk Analysis
            if hasattr(self, 'regional_risk_scores') and len(self.regional_risk_scores) > 1:
                regions = list(self.regional_risk_scores.keys())
                regional_scores = list(self.regional_risk_scores.values())
                
                # Sort by risk score
                sorted_data = sorted(zip(regions, regional_scores), key=lambda x: x[1], reverse=True)
                regions, regional_scores = zip(*sorted_data)
                
                colors = plt.cm.viridis(np.linspace(0, 1, len(regions)))
                bars = self.axes[1, 1].bar(range(len(regions)), regional_scores, color=colors)
                self.axes[1, 1].set_title('Regional Risk Scores\n(Ranked by Risk Level)', fontweight='bold', fontsize=12)
                self.axes[1, 1].set_ylabel('Risk Score')
                self.axes[1, 1].set_xticks(range(len(regions)))
                self.axes[1, 1].set_xticklabels(regions, rotation=45, ha='right')
                
                # Add value labels
                for bar, score in zip(bars, regional_scores):
                    height = bar.get_height()
                    self.axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                                       f'{score:.3f}', ha='center', va='bottom', fontsize=8)
            
            # Chart 6: Stratification Dimensions Impact Analysis
            if hasattr(self, 'sampling_parameters'):
                dimensions = ['Entity', 'Region', 'Product'] + self.sampling_parameters.get('optional_cols', [])
                dimension_counts = []
                
                for dim in dimensions:
                    if dim.lower() in ['entity', 'legal_entity']:
                        count = len(self.entity_risk_scores)
                    elif dim.lower() in ['region']:
                        count = len(self.regional_risk_scores)
                    elif dim.lower() in ['product', 'product_type']:
                        count = len(self.product_weights)
                    elif dim in self.additional_risk_weights:
                        count = len(self.additional_risk_weights[dim]['weights'])
                    else:
                        count = 0
                    dimension_counts.append(count)
                
                colors = plt.cm.Set3(np.linspace(0, 1, len(dimensions)))
                wedges, texts, autotexts = self.axes[1, 2].pie(dimension_counts, labels=dimensions, 
                                                              colors=colors, autopct='%1.0f', 
                                                              startangle=90)
                self.axes[1, 2].set_title('Stratification Dimension\nCardinality Distribution', fontweight='bold', fontsize=12)
                
                # Enhance text appearance
                for autotext in autotexts:
                    autotext.set_color('white')
                    autotext.set_fontweight('bold')
            
        except Exception as e:
            print(f"Error creating comprehensive charts: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def create_coverage_charts(self):
        """Create focused coverage analysis charts"""
        # Implementation for coverage-specific charts
        pass
    
    def create_distribution_charts(self):
        """Create distribution analysis charts"""
        # Implementation for distribution-specific charts
        pass
    
    def create_risk_analysis_charts(self):
        """Create risk analysis charts"""
        # Implementation for risk analysis charts
        pass
    
    def create_stratum_heatmap(self):
        """Create stratum-level heatmap visualization"""
        # Implementation for stratum heatmap
        pass
    
    def export_out_of_scope_exceptions(self):
        """Export exceptions not included in any sample with detailed analysis"""
        
        if not self.comparison_results:
            messagebox.showerror("Error", "No sampling results to export")
            return
        
        try:
            # Get all sampled indices across all methods
            sampled_indices = set()
            method_coverage = {}
            
            for method_name, sample in self.comparison_results.items():
                sampled_indices.update(sample.index)
                method_coverage[method_name] = set(sample.index)
            
            # Get out-of-scope exceptions
            out_of_scope = self.data[~self.data.index.isin(sampled_indices)].copy()
            
            if len(out_of_scope) > 0:
                # Enhanced analysis of out-of-scope items
                out_of_scope['exclusion_reason'] = "Not selected in any sampling method"
                
                # Add detailed exclusion analysis
                out_of_scope = self.ensure_numeric_column(out_of_scope, 'risk_score')
                out_of_scope['risk_category'] = out_of_scope['risk_score'].apply(
                    lambda x: 'High' if x > 0.7 else ('Medium' if x > 0.4 else 'Low')
                )
                
                # Add stratum analysis
                if hasattr(self, 'last_stratum_samples'):
                    out_of_scope['stratum_sampled'] = out_of_scope['stratum'].apply(
                        lambda x: 'Yes' if x in self.last_stratum_samples else 'No'
                    )
                    
                    def get_stratum_info(row):
                        stratum = row['stratum']
                        if stratum in self.stratum_details:
                            details = self.stratum_details[stratum]
                            return f"Pop: {details['population']}, Sampled: {details['final_sample']}"
                        return "Unknown stratum"
                    
                    out_of_scope['stratum_info'] = out_of_scope.apply(get_stratum_info, axis=1)
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"omrc_out_of_scope_exceptions_{timestamp}.csv"
                
                file_path = filedialog.asksaveasfilename(
                    title="Save Out-of-Scope Exceptions",
                    defaultextension=".csv",
                    filetypes=[("CSV files", "*.csv")],
                    initialvalue=filename
                )
                
                if file_path:
                    out_of_scope.to_csv(file_path, index=False)
                    
                    # Create summary report
                    summary_report = f"""
OUT-OF-SCOPE EXCEPTIONS ANALYSIS SUMMARY
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

OVERVIEW:
Total Population: {len(self.data):,}
Total Sampled (Any Method): {len(sampled_indices):,}
Out-of-Scope: {len(out_of_scope):,} ({len(out_of_scope)/len(self.data)*100:.1f}%)

RISK BREAKDOWN OF OUT-OF-SCOPE:
High Risk (>0.7): {len(out_of_scope[out_of_scope['risk_score'] > 0.7]):,}
Medium Risk (0.4-0.7): {len(out_of_scope[(out_of_scope['risk_score'] >= 0.4) & (out_of_scope['risk_score'] <= 0.7)]):,}
Low Risk (<0.4): {len(out_of_scope[out_of_scope['risk_score'] < 0.4]):,}

SAMPLING METHOD COVERAGE COMPARISON:
"""
                    for method, indices in method_coverage.items():
                        coverage_pct = len(indices) / len(self.data) * 100
                        summary_report += f"{method.replace('_', ' ').title()}: {len(indices):,} ({coverage_pct:.1f}%)\n"
                    
                    # Save summary alongside the data
                    summary_path = file_path.replace('.csv', '_summary.txt')
                    with open(summary_path, 'w') as f:
                        f.write(summary_report)
                    
                    messagebox.showinfo("Success", 
                        f"Exported {len(out_of_scope):,} out-of-scope exceptions\n\n"
                        f"Files created:\n"
                        f" {os.path.basename(file_path)} (detailed data)\n"
                        f" {os.path.basename(summary_path)} (summary analysis)")
            else:
                messagebox.showinfo("Info", "No out-of-scope exceptions to export - all records were sampled")
                
        except Exception as e:
            messagebox.showerror("Error", f"Failed to export out-of-scope exceptions: {str(e)}")
    
    def export_samples(self):
        """Export enhanced sample data to CSV with metadata"""
        
        if not self.comparison_results:
            messagebox.showerror("Error", "No samples to export")
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            for method_name, sample in self.comparison_results.items():
                if len(sample) > 0:
                    # Enhanced sample with metadata
                    sample_enhanced = sample.copy()
                    sample_enhanced = self.ensure_numeric_column(sample_enhanced, 'risk_score')
                    sample_enhanced['sampling_method'] = method_name.replace('_', ' ').title()
                    sample_enhanced['sample_timestamp'] = timestamp
                    sample_enhanced['risk_category'] = sample_enhanced['risk_score'].apply(
                        lambda x: 'High' if x > 0.7 else ('Medium' if x > 0.4 else 'Low')
                    )
                    
                    filename = f"omrc_enhanced_{method_name}_sample_{timestamp}.csv"
                    file_path = filedialog.asksaveasfilename(
                        title=f"Save {method_name} Sample",
                        defaultextension=".csv",
                        filetypes=[("CSV files", "*.csv")],
                        initialvalue=filename
                    )
                    
                    if file_path:
                        sample_enhanced.to_csv(file_path, index=False)
                        
            messagebox.showinfo("Success", "Enhanced multi-dimensional samples exported successfully")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to export samples: {str(e)}")
    
    def export_report(self):
        """Export comprehensive enhanced comparison report"""
        
        if not self.comparison_results:
            messagebox.showerror("Error", "No analysis to export")
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"omrc_enhanced_multi_dimensional_report_{timestamp}.txt"
            
            file_path = filedialog.asksaveasfilename(
                title="Save Enhanced Multi-Dimensional Report",
                defaultextension=".txt",
                filetypes=[("Text files", "*.txt")],
                initialvalue=filename
            )
            
            if file_path:
                # Comprehensive report
                exec_content = self.exec_text.get(1.0, tk.END)
                results_content = self.results_text.get(1.0, tk.END)
                
                full_report = "OMRC ENHANCED MULTI-DIMENSIONAL STATISTICAL SAMPLING REPORT\n"
                full_report += "=" * 80 + "\n\n"
                full_report += exec_content + "\n\n"
                full_report += "DETAILED TECHNICAL ANALYSIS:\n"
                full_report += "=" * 50 + "\n"
                full_report += results_content
                
                # Add enhanced metadata
                if hasattr(self, 'sampling_parameters'):
                    full_report += f"\n\nENHANCED METHODOLOGY METADATA:\n"
                    full_report += f"Stratification Approach: Multi-Dimensional Enhanced\n"
                    full_report += f"Total Dimensions: {self.sampling_parameters.get('stratification_dimensions', 'N/A')}\n"
                    full_report += f"Risk Weighting: Dynamic Statistical Frequency-Based\n"
                    full_report += f"Anomaly Detection: Isolation Forest with {self.sampling_parameters.get('stratification_dimensions', 'N/A')} features\n"
                
                with open(file_path, 'w') as f:
                    f.write(full_report)
                
                messagebox.showinfo("Success", f"Enhanced multi-dimensional report exported to {file_path}")
                
        except Exception as e:
            messagebox.showerror("Error", f"Failed to export report: {str(e)}")
    
    def export_risk_analysis(self):
        """Export comprehensive risk analysis with all dimensions"""
        
        if not hasattr(self, 'entity_risk_scores'):
            messagebox.showerror("Error", "No risk analysis to export")
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"omrc_enhanced_risk_weights_{timestamp}.csv"
            
            file_path = filedialog.asksaveasfilename(
                title="Save Enhanced Risk Weights Analysis",
                defaultextension=".csv",
                filetypes=[("CSV files", "*.csv")],
                initialvalue=filename
            )
            
            if file_path:
                # Comprehensive risk data
                risk_data = []
                
                # Mandatory dimensions
                dimensions_data = [
                    ('Legal Entity', 'Mandatory', self.entity_risk_scores, self.entity_counts, self.entity_frequencies),
                    ('Region', 'Mandatory', self.regional_risk_scores, self.regional_counts, self.regional_frequencies),
                    ('Product Type', 'Mandatory', self.product_weights, self.product_counts, self.product_frequencies),
                    ('Reason Code', 'Mandatory', self.reason_code_weights, self.reason_counts, self.reason_frequencies)
                ]
                
                # Add optional dimensions
                for col, risk_data_dict in self.additional_risk_weights.items():
                    dimensions_data.append((
                        col, 'Optional', 
                        risk_data_dict['weights'], 
                        risk_data_dict['counts'], 
                        risk_data_dict['frequencies']
                    ))
                
                # Process all dimensions
                for category, dim_type, weights, counts, frequencies in dimensions_data:
                    for item, weight in weights.items():
                        count = counts.get(item, 0)
                        freq = frequencies.get(item, 0)
                        risk_data.append({
                            'Dimension_Type': dim_type,
                            'Category': category,
                            'Item': item,
                            'Exception_Count': count,
                            'Frequency_Percent': freq * 100,
                            'Statistical_Weight': weight,
                            'Risk_Level': 'High' if weight > 0.7 else ('Medium' if weight > 0.4 else 'Low')
                        })
                
                df_risk = pd.DataFrame(risk_data)
                
                # Add summary statistics
                summary_stats = []
                for category in df_risk['Category'].unique():
                    cat_data = df_risk[df_risk['Category'] == category]
                    summary_stats.append({
                        'Dimension_Type': 'SUMMARY',
                        'Category': f'{category}_SUMMARY',
                        'Item': 'STATISTICS',
                        'Exception_Count': cat_data['Exception_Count'].sum(),
                        'Frequency_Percent': cat_data['Frequency_Percent'].sum(),
                        'Statistical_Weight': cat_data['Statistical_Weight'].mean(),
                        'Risk_Level': f"Items: {len(cat_data)}, Avg: {cat_data['Statistical_Weight'].mean():.3f}"
                    })
                
                df_final = pd.concat([df_risk, pd.DataFrame(summary_stats)], ignore_index=True)
                df_final.to_csv(file_path, index=False)
                
                messagebox.showinfo("Success", f"Enhanced multi-dimensional risk analysis exported to {file_path}")
                
        except Exception as e:
            messagebox.showerror("Error", f"Failed to export risk analysis: {str(e)}")
    
    def export_stratum_analysis(self):
        """Export enhanced stratum analysis with multi-dimensional breakdown"""
        
        if not hasattr(self, 'stratum_details') or not self.stratum_details:
            messagebox.showerror("Error", "No stratum analysis to export")
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"omrc_enhanced_stratum_analysis_{timestamp}.csv"
            
            file_path = filedialog.asksaveasfilename(
                title="Save Enhanced Stratum Analysis",
                defaultextension=".csv",
                filetypes=[("CSV files", "*.csv")],
                initialvalue=filename
            )
            
            if file_path:
                # Enhanced stratum data
                stratum_data = []
                mandatory_cols, optional_cols = self.get_stratification_columns()
                
                for stratum_name, details in self.stratum_details.items():
                    # Parse stratum components
                    if isinstance(stratum_name, tuple):
                        stratum_components = list(stratum_name)
                    else:
                        stratum_components = str(stratum_name).split('_')
                    
                    # Create row data
                    row_data = {
                        'Stratum_ID': str(stratum_name),
                        'Stratum_Population': details['population'],
                        'Risk_Weight': details['risk_weight'],
                        'Base_Sample_Size': details['base_sample'],
                        'Risk_Adjusted_Sample': details['risk_adjusted'],
                        'Final_Sample_Size': details['final_sample'],
                        'Sample_Rate_Percent': (details['final_sample'] / details['population'] * 100) if details['population'] > 0 else 0,
                        'Dimensions_Used': details.get('dimensions_used', len(mandatory_cols) + len(optional_cols)),
                        'Stratification_Efficiency': details['final_sample'] / details['base_sample'] if details['base_sample'] > 0 else 1
                    }
                    
                    # Add dimensional breakdowns
                    all_cols = mandatory_cols + optional_cols
                    for i, col in enumerate(all_cols):
                        if i < len(stratum_components):
                            row_data[f'{col}'] = stratum_components[i]
                        else:
                            row_data[f'{col}'] = 'N/A'
                    
                    stratum_data.append(row_data)
                
                df_stratum = pd.DataFrame(stratum_data)
                
                # Sort by population size (descending)
                df_stratum = df_stratum.sort_values('Stratum_Population', ascending=False)
                
                df_stratum.to_csv(file_path, index=False)
                
                messagebox.showinfo("Success", f"Enhanced multi-dimensional stratum analysis exported to {file_path}")
                
        except Exception as e:
            messagebox.showerror("Error", f"Failed to export stratum analysis: {str(e)}")
    
    def log_results(self, message):
        """Log message to results text area"""
        self.results_text.insert(tk.END, message + "\n")
        self.results_text.see(tk.END)

# Main execution
if __name__ == "__main__":
    root = tk.Tk()
    app = OMRCRiskBasedSamplingTool(root)
    root.mainloop()
